# -*- coding: utf-8 -*-
"""VITALE AI functions.ipynb

Automatically generated by Colab.




1. Install dependencies:
```bash
pip install -r requirements.txt

requirements.txt:
fastapi
uvicorn
openai==0.28
sentence-transformers
pyspark
numpy
pandas
scikit-learn
python-dotenv


pubmed-vitale/
│
├── app.py                       #  POST /query
├── biobert_ranker.py           #  get_top_k_articles_by_cosine_similarity()
├── openai_summarizer.py        #  summarize_with_openai() + prompt
├── utils.py                    #  Spark loader, embeddings, pmids, PubMed_Vitale.parque
│
├── data/   =======> C:\Users\User\EFEVRE TECH LTD Dropbox\99 PROGRAM & REPORT\Rafaella Hadjicosti\4. Proposals\00. APPROVED PROPOSALS\ENTERPRISES-0223_SubCall-1-0105\03. SOFTWARE TEAM\CONSTANTINOS\DATA&EMBEDDINGS
│   ├── PubMed_Vitale.parquet
│   ├── PubMed_Vitale_pmids.csv
│   └── PubMed_Vitale_embeddings_BioBert.npy
│
├── .env                        #  OPENAI_API_KEY= "YOUR_API_KEY_HERE"
├── requirements.txt            #  All packages
└── README.md                   #  instructions




| `biobert_ranker.py` |  **Semantic ranking module.** Provides `get_top_k_articles_by_cosine_similarity(query, k)` that uses BioBERT embeddings and cosine similarity to find the top relevant PubMed documents. |
| `openai_summarizer.py` |  **Summarization module.** Implements `summarize_with_openai(query, abstracts)`, which sends the top abstracts to GPT-4 with a structured biomedical prompt. |
| `utils.py` |  **Utilities module.** Handles SparkSession creation and loads core datasets (e.g., `PubMed_Vitale.parquet`, embeddings, PMIDs). Shared across modules. |
| `data/PubMed_Vitale.parquet` |  **Main dataset file.** A Spark-compatible Parquet file containing structured metadata for each PubMed article. |
| `data/PubMed_Vitale_pmids.csv` |  **PMID index.** A list of PMIDs corresponding to the order of embeddings, used for fast lookups. |
| `data/PubMed_Vitale_embeddings_BioBert.npy` |  **Precomputed BioBERT embeddings** for each article's overview text, stored in `.npy` format for fast loading. |


Original file is located at
    https://colab.research.google.com/drive/1IaFXvKBpWOwE7ouIapYWgM4AxTUtjD_s
"""

from google.colab import drive
drive.mount('/content/drive')

# === Setup ===
# Load Spark session and data only once
import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import ipywidgets as widgets
from IPython.display import display, clear_output
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


# Load once on server start
spark = SparkSession.builder.appName("PubMedVitaleWebApp").getOrCreate()
df_pubmed_details = spark.read.parquet("/content/drive/MyDrive/PubMed_Vitale.parquet")

pmids_df = pd.read_csv("/content/drive/MyDrive/PubMed_Vitale_pmids.csv")
pmids = pmids_df["PMID"].astype(str).tolist()
embeddings = np.load("/content/drive/MyDrive/PubMed_Vitale_embeddings_BioBert.npy")

model = SentenceTransformer("pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb")

# === Core Function ===
def get_top_k_articles_by_cosine_similarity(query: str, k: int = 20):
    """
    Rank documents by cosine similarity to the user query using BioBERT embeddings.

    Args:
        query (str): The user input query.
        k (int): Number of top documents to return.

    Returns:
        Spark DataFrame: Top-k ranked PubMed entries with cosine similarity score.
    """
    query_embedding = model.encode([query])[0]
    scores = cosine_similarity([query_embedding], embeddings)[0]

    top_indices = np.argsort(scores)[::-1][:k]
    top_pmids = [pmids[i] for i in top_indices]
    top_scores = [float(scores[i]) for i in top_indices]

    # Build temporary Spark DataFrame with scores
    df_scores = pd.DataFrame({"PMID": top_pmids, "cosine_similarity": top_scores})
    df_scores_spark = spark.createDataFrame(df_scores)

    # Join with pubmed metadata
    df_result = df_scores_spark.join(df_pubmed_details, on="PMID", how="inner")
    df_result = df_result.orderBy(col("cosine_similarity").desc())

    return df_result

#Test the ranking based on the query
if __name__ == "__main__":
    query = "cytokine signaling in autoimmune diseases"
    top_k_df = get_top_k_articles_by_cosine_similarity(query, k=20)
    top_k_df.select("PMID", "Title", "cosine_similarity").show(truncate=False)

"""No need to show the ranking table."""

# =========
# Install in Colab
# =========
!pip install openai==0.28

import openai
import os

# -*- coding: utf-8 -*-
"""
openai_summarizer.py
Module that uses OpenAI GPT to summarize top ranked biomedical abstracts retrieved from PubMed.
"""

import openai
import os
from typing import List
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Get API key from environment variable
openai.api_key = os.getenv("OPENAI_API_KEY", "")

"""You can fine tune the gpt-4 with prompt engineering. Ask Dimitris."""

def summarize_with_openai(query, abstracts, model="gpt-4"):
    """
    Given a user query and a list of top-ranked abstracts,
    generate a summarized answer using OpenAI GPT.

    Args:
        query (str): User’s original query.
        abstracts (List[str]): List of top abstracts.
        model (str): OpenAI model name.

    Returns:
        str: Summarized GPT-generated answer.
    """
    formatted_abstracts = "\n\n".join([f"{i+1}. {a}" for i, a in enumerate(abstracts)])

    prompt = f"""
You are a biomedical research assistant. A user asked:

"{query}"

Here are the most relevant research abstracts from PubMed:

{formatted_abstracts}

Carefully analyze the user's query and understand its full semantic intent.

When reviewing each abstract, identify sentences that are semantically related to the user's question — not just by keywords, but by meaning and context.

Structure your output as follows:
1. Introductory clarification or definition – Briefly explain any central concept if it is not common knowledge and is critical to understanding the findings.
2. Main findings – Present clear, evidence-based bullet points or nested points. Ensure smooth transitions between topics and highlight relationships relative to the query.
3. Integrated summary or conclusion – Synthesize the insights into a short paragraph that connects the findings and their implications in relation to the user's query.

Focus your summary only on the findings that directly support or expand on the core idea of the query. Discard content that is unrelated or tangential, even if it shares some vocabulary.

Based on these abstracts, summarize the key findings that relate to the user's question.

If applicable highlight biological pathways, molecular functions, biological processes, cellular components, genes, or GO terms.

Do not include information that is not present or supported in the text.

Present the summary in bullet points or structured form.

Avoid unnecessary repetition; present new insight at each stage of the summary.

Your tone should reflect professional scientific communication, suitable for a biomedical research audience.

"""

    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=1000
    )

    return response['choices'][0]['message']['content']

# User query
query = "cytokine signaling in autoimmune diseases"

# Step 1: Get top articles by cosine similarity
df_ranked = get_top_k_articles_by_cosine_similarity(query, k=3)
df_pd = df_ranked.select("PMID", "overview_text").toPandas()

# Step 2: Extract abstracts in ranked order
abstracts = df_pd["overview_text"].fillna("").tolist()

# Step 3: Summarize using OpenAI
summary = summarize_with_openai(query, abstracts)

# Display result
print("Query:", query)
print("\n GPT Summary:\n")
print(summary)






# -*- coding: utf-8 -*-
"""
app.py
Main FastAPI application for serving PubMed-VITALE AI search & summarization.

This file imports and uses the following core functions:
- `get_top_k_articles_by_cosine_similarity()` from `biobert_ranker.py`:
    Retrieves the top-k most relevant PubMed articles using BioBERT-based semantic search.
- `summarize_with_openai()` from `openai_summarizer.py`:
    Generates a structured scientific summary of the top-ranked articles using GPT-4.

The API exposes a single `/query` endpoint for end-to-end biomedical question answering.
"""
"""

from fastapi import FastAPI
from pydantic import BaseModel
from biobert_ranker import get_top_k_articles_by_cosine_similarity
from openai_summarizer import summarize_with_openai

# Initialize FastAPI app
app = FastAPI(
    title="VITALE Semantic Search API",
    description="Semantic PubMed search and summarization using BioBERT + GPT-4",
    version="1.0.0"
)

# Define input schema
class Query(BaseModel):
    query: str

@app.post("/query")
def query_pubmed(data: Query):
    """
    Accepts a biomedical search query, retrieves top documents,
    summarizes them using GPT, and returns structured insights.

    Args:
        data (Query): JSON body with 'query' string.

    Returns:
        dict: Dictionary containing the query and GPT summary.
    """
    query = data.query

    # Step 1: Retrieve top-k relevant documents
    df = get_top_k_articles_by_cosine_similarity(query, k=4)

    # Step 2: Extract abstract-like content (overview)
    abstracts = df.select("overview_text").toPandas()["overview_text"].fillna("").tolist()

    # Step 3: Summarize using OpenAI GPT
    summary = summarize_with_openai(query, abstracts)

    # Step 4: Return result
    return {
        "query": query,
        "summary": summary
    }

